{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Integrated_Chat.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM8gCdoBTHMqv0d4T1uq3AZ"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"CBWnG-KNI2-F"},"source":["from google.colab import files\r\n","import os\r\n","from google.colab import drive\r\n","drive.mount('/content/drive')\r\n","os.chdir('/content/drive/My Drive/Bot_sides')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qh1_YwqRI-PF"},"source":["import nltk\r\n","nltk.download('punkt')\r\n","nltk.download('wordnet')\r\n","from nltk.stem import WordNetLemmatizer\r\n","lemmatizer = WordNetLemmatizer()\r\n","import pickle\r\n","import numpy as np\r\n","\r\n","from keras.models import load_model\r\n","conversation_model = load_model('chatbot_model.h5')\r\n","import json\r\n","import random\r\n","intents = json.loads(open('intents.json').read())\r\n","words = pickle.load(open('words.pkl','rb'))\r\n","classes = pickle.load(open('classes.pkl','rb'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v8tRGiEKJB6d"},"source":["import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6jMI8GywJVIw"},"source":["def clean_up_sentence(sentence):\r\n","    # tokenize the pattern - split words into array\r\n","    sentence_words = nltk.word_tokenize(sentence)\r\n","    # stem each word - create short form for word\r\n","    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\r\n","    return sentence_words\r\n","# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\r\n","\r\n","def bow(sentence, words, show_details=True):\r\n","    # tokenize the pattern\r\n","    sentence_words = clean_up_sentence(sentence)\r\n","    # bag of words - matrix of N words, vocabulary matrix\r\n","    bag = [0]*len(words) \r\n","    for s in sentence_words:\r\n","        for i,w in enumerate(words):\r\n","            if w == s: \r\n","                # assign 1 if current word is in the vocabulary position\r\n","                bag[i] = 1\r\n","                if show_details:\r\n","                    print (\"found in bag: %s\" % w)\r\n","    return(np.array(bag))\r\n","\r\n","def predict_class(sentence, model):\r\n","    # filter out predictions below a threshold\r\n","    p = bow(sentence, words,show_details=False)\r\n","    res = conversation_model.predict(np.array([p]))[0]\r\n","    ERROR_THRESHOLD = 0.25\r\n","    results = [[i,r] for i,r in enumerate(res)]\r\n","    # sort by strength of probability\r\n","    results.sort(key=lambda x: x[1], reverse=True)\r\n","    return_list = []\r\n","    for r in results:\r\n","        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\r\n","    return return_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_JpDd0XeKLEk"},"source":["import matplotlib.pyplot as plt\r\n","import seaborn as sns\r\n","import spacy\r\n","from tqdm import tqdm, tqdm_notebook, tnrange\r\n","tqdm.pandas(desc='Progress')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EKyDZrR1KaGy"},"source":["import torchtext\r\n","from torchtext.data import Field, BucketIterator, TabularDataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9rav9u4xKeIS"},"source":["from sklearn.model_selection import train_test_split\r\n","from sklearn.metrics import accuracy_score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T-USSw3PKha9"},"source":["import os, sys\r\n","import re\r\n","import string\r\n","import itertools"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uCdqaG6SKjQ5"},"source":["import torch\r\n","import torch.nn as nn\r\n","import torch.optim as optim\r\n","from torch.autograd import Variable\r\n","import torch.nn.functional as F\r\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Im8Zwb6KlJt"},"source":["contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cyst10d7KnBv"},"source":["def _get_contractions(contraction_dict):\r\n","    contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\r\n","    return contraction_dict, contraction_re\r\n","\r\n","contractions, contractions_re = _get_contractions(contraction_dict)\r\n","\r\n","def replace_contractions(text):\r\n","    def replace(match):\r\n","        return contractions[match.group(0)]\r\n","    return contractions_re.sub(replace, text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fBqVJBsnKpUF"},"source":["def text_clean(text):\r\n","    text = re.sub(r'https?:/\\/\\S+', ' ', text) # remove urls\r\n","    text = re.sub(r'<([^>]*)>', ' ', text) # remove emojis\r\n","    text = re.sub(r'@\\w+', ' ', text) # remove at mentions\r\n","    text = re.sub(r'#', '', text) # remove hashtag symbol\r\n","    text = re.sub(r'[0-9]+', ' ', text) # remove numbers\r\n","    text = replace_contractions(text)\r\n","    pattern = re.compile(r\"[ \\n\\t]+\")\r\n","    text = pattern.sub(\" \", text)      \r\n","    text = \"\".join(\"\".join(s)[:2] for _, s in itertools.groupby(text))    \r\n","    text = re.sub(r'[^A-Za-z0-9,?.!]+', ' ', text) # remove all symbols and punctuation except for . , ! and ?\r\n","    return text.strip()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"oipdDibcOoen","executionInfo":{"status":"ok","timestamp":1611304499794,"user_tz":300,"elapsed":935,"user":{"displayName":"Subha Nawer Pushpita","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIHjytx_rOqP46_t7c300pEKvxMOBeWfsQZH6MJw=s64","userId":"15148330505862032702"}},"outputId":"318cd45a-68fa-4373-b627-8fd18b060523"},"source":["text_clean('I love myself')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'I love myself'"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"LTuHrNleKrbC"},"source":["nlp = spacy.load('en',disable=['parser', 'tagger', 'ner'])\r\n","def tokenizer(s): return [w.text.lower() for w in nlp(text_clean(s))]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WNrc4ptAKtpr"},"source":["TEXT = Field(sequential=True, tokenize=tokenizer, include_lengths=True, use_vocab=True)\r\n","TARGET = Field(sequential=False, use_vocab=False, pad_token=None, unk_token=None, is_target =False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xJa3xsZBKvmr"},"source":["data_fields = [\r\n","    (None, None),\r\n","    (\"tweet\", TEXT), \r\n","    (\"target\", TARGET)\r\n","]\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"urfAefcfL1G8"},"source":["vec = torchtext.vocab.Vectors('glove.6B.100d.txt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8sXa3eYQL51k"},"source":["train_data, val_data, test_data = TabularDataset.splits(path='./', format='csv', train='train.csv', validation='val.csv', test='test.csv', fields=data_fields, skip_header=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"45MRRS95L7rg"},"source":["MAX_VOCAB_SIZE = 100_000\r\n","\r\n","TEXT.build_vocab(train_data, \r\n","                 max_size = MAX_VOCAB_SIZE,\r\n","                 vectors=vec)\r\n","\r\n","TARGET.build_vocab(train_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R8YywqtkL9yY"},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HBikROL-MATz"},"source":["def read_vocab(path):\r\n","    #read vocabulary pkl \r\n","    import pickle\r\n","    pkl_file = open(path, 'rb')\r\n","    vocab = pickle.load(pkl_file)\r\n","    pkl_file.close()\r\n","    return vocab"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QGqH1ZtRMCWy"},"source":["vocab_path = 'vocab.pkl'\r\n","vocab = read_vocab(vocab_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"woazDUwsMESx"},"source":["vocab_size = len(TEXT.vocab)\r\n","embedding_dim = 100\r\n","n_hidden = 64\r\n","n_out = 2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5yJ15_3ZMGT6"},"source":["class ConcatPoolingGRUAdaptive(nn.Module):\r\n","    def __init__(self, vocab_size, embedding_dim, n_hidden, n_out, pretrained_vec, dropout, bidirectional=True):\r\n","        super().__init__()\r\n","        self.vocab_size = vocab_size\r\n","        self.embedding_dim = embedding_dim\r\n","        self.n_hidden = n_hidden\r\n","        self.n_out = n_out\r\n","        self.bidirectional = bidirectional\r\n","        \r\n","        self.emb = nn.Embedding(self.vocab_size, self.embedding_dim)\r\n","        self.emb.weight.data.copy_(pretrained_vec)\r\n","        self.emb.weight.requires_grad = False\r\n","        self.gru = nn.GRU(self.embedding_dim, self.n_hidden, bidirectional=bidirectional)\r\n","        if bidirectional:\r\n","            self.fc = nn.Linear(self.n_hidden*2*2, self.n_out)\r\n","        else:\r\n","            self.fc = nn.Linear(self.n_hidden*2, self.n_out)\r\n","        self.dropout = nn.Dropout(dropout)\r\n","        \r\n","    def forward(self, seq, lengths):\r\n","        bs = seq.size(1)\r\n","        self.h = self.init_hidden(bs)\r\n","        seq = seq.transpose(0,1)\r\n","        embs = self.emb(seq)\r\n","        embs = embs.transpose(0,1)\r\n","        embs = pack_padded_sequence(embs, lengths)\r\n","        gru_out, self.h = self.gru(embs, self.h)\r\n","        gru_out, lengths = pad_packed_sequence(gru_out)        \r\n","        \r\n","        avg_pool = F.adaptive_avg_pool1d(gru_out.permute(1,2,0),1).view(bs,-1)\r\n","        max_pool = F.adaptive_max_pool1d(gru_out.permute(1,2,0),1).view(bs,-1) \r\n","        \r\n","        cat = self.dropout(torch.cat([avg_pool,max_pool],dim=1))\r\n","        \r\n","        outp = self.fc(cat)\r\n","        return F.log_softmax(outp)\r\n","    \r\n","    def init_hidden(self, batch_size): \r\n","        if self.bidirectional:\r\n","            return torch.zeros((2,batch_size,self.n_hidden)).to(device)\r\n","        else:\r\n","            return torch.zeros((1,batch_size,self.n_hidden)).cuda().to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-ZSTDgjxMI7Z"},"source":["depression_model = ConcatPoolingGRUAdaptive(vocab_size, embedding_dim, n_hidden, n_out, train_data.fields['tweet'].vocab.vectors, 0.5).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xxliXnsLMKz7"},"source":["depression_model.load_state_dict(torch.load('tut4-model.pt'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9awXgrpKMTge"},"source":["import numpy as np\r\n","\r\n","def predict(model, vocab, sentence):\r\n","    tokenized = [w.text.lower() for w in nlp(text_clean(sentence))]  # tokenize the sentence\r\n","    if len(tokenzied)<=4:\r\n","        return 'N/A',0\r\n","    indexed = [vocab.stoi[t] for t in tokenized]                     # convert to integer sequence\r\n","    length = [len(indexed)]                                          # compute no. of words\r\n","    tensor = torch.LongTensor(indexed).to(device)                    # convert to tensor\r\n","    tensor = torch.reshape(tensor, (length[0], 1))                   # reshape in form of batch,no. of words\r\n","    length_tensor = torch.LongTensor(length)\r\n","    prediction = model(tensor, length_tensor) \r\n","    print(prediction)\r\n","    n_prediction = prediction.detach().cpu().numpy()\r\n","    numerator = np.exp(n_prediction[0,0])+np.exp(n_prediction[0,1])\r\n","    zero_score = np.exp(n_prediction[0,0])/numerator\r\n","    one_score = np.exp(n_prediction[0,1])/numerator\r\n","    if zero_score>one_score:\r\n","        print('Non Depressive',zero_score)\r\n","    else:\r\n","        print('Depressive',one_score)\r\n","   \r\n","    #print(torch.max(prediction,1))\r\n","    #pred_idx = torch.max(prediction, 1)[1]\r\n","    #if pred_idx == torch.tensor([0]).to(device):\r\n","    #    print('Non Depressive')\r\n","    #else:\r\n","    #    print('Depressive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R-Hur4rkMmel"},"source":["def getResponse(ints, intents_json,index):\r\n","    tag = ints[index]['intent']\r\n","    list_of_intents = intents_json['intents']\r\n","    for i in list_of_intents:\r\n","        if(i['tag']== tag):\r\n","            result = random.choice(i['responses'])\r\n","            break\r\n","    return tag,result\r\n","\r\n","def chatbot_response(current_tags,text):\r\n","    ints = predict_class(text, conversation_model)\r\n","    l = len(ints)\r\n","    index = 0\r\n","    for i in range(l):\r\n","        if ints[i]['intent'] not in current_tags:\r\n","            index=i\r\n","            break\r\n","    new_tag, res = getResponse(ints, intents,index)\r\n","    return new_tag,res"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UYY40O4sMtqu"},"source":["def chat():\r\n","    tag_dict = {\"start\",\"feel\",\"sleep\",\"work\",\"physical health\",\"confidence\",\"suicidal thoughts\"}\r\n","    depression_score = 0\r\n","    count = 0\r\n","    current_tags = set()\r\n","    print(\"Start talking with the bot (type quit to stop)!\")\r\n","    while True:\r\n","        inp = input(\"You: \")\r\n","        if inp.lower() == \"quit\":\r\n","            avg = depression_score/count\r\n","            print('Your texts sound {}% depressed'.format())\r\n","            break\r\n","        string, score = predict(depression_model, vocab, sentence)\r\n","        if score>0:\r\n","            if string=='Non Depressive':\r\n","                depression_score += (1-score)\r\n","            else:\r\n","                depression_score += score\r\n","            count+=1\r\n","        new_tag,result = chatbot_response(current_tags,inp)\r\n","        current_tags.add(new_tag)\r\n","        if new_tag=='result':            \r\n","            print('')\r\n","        print('Maya:{}'.format(result))\r\n","chat()"],"execution_count":null,"outputs":[]}]}